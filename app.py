# # from flask import Flask, request, jsonify
# # import requests, os, json, re

# # app = Flask(__name__)

# # # ğŸ”‘ Gemini API Configuration
# # GEMINI_API_KEY = "AIzaSyA1tOLp9zmbiBprpuhQZqq7s6TERss4x7s"
# # GEMINI_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={GEMINI_API_KEY}"


# # # ğŸ§  Helper: Build Prompt
# # def build_prompt(exam_type, grade, subject, count, lang="en", task="generate", answers=None, student_id=""):
# #     if task == "generate":
# #         if lang == "ar":
# #             return f"""
# #             Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª.
# #             Ø§Ù„Ù…Ù‡Ù…Ø©: Ø£Ù†Ø´Ø¦ {count} Ø£Ø³Ø¦Ù„Ø© ÙØ±ÙŠØ¯Ø© ÙˆÙ…Ø¨ØªÙƒØ±Ø© Ù„Ø§Ù…ØªØ­Ø§Ù† {exam_type}.
# #             Ø§Ù„ØµÙ: {grade}
# #             Ø§Ù„Ù…Ø§Ø¯Ø©: {subject}
# #             âœ… Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯:
# #             - Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø£Ø³Ø¦Ù„Ø© Ù…ÙƒØ±Ø±Ø©.
# #             - ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ù…ØªÙ†ÙˆØ¹Ø© ÙˆÙ…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ù…Ø³ØªÙˆÙ‰.
# #             - ÙƒÙ„ Ø³Ø¤Ø§Ù„ ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰:
# #               "id": Ø±Ù‚Ù…,
# #               "question": "Ù†Øµ Ø§Ù„Ø³Ø¤Ø§Ù„",
# #               "options": ["Ø£", "Ø¨", "Ø¬", "Ø¯"],
# #               "correct_answer": "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©"
# #             - Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø³ØªÙˆÙŠØ§Øª ØµØ¹ÙˆØ¨Ø© Ù…Ø®ØªÙ„ÙØ© (Ø³Ù‡Ù„ØŒ Ù…ØªÙˆØ³Ø·ØŒ ØµØ¹Ø¨).
# #             - Ø£Ø±Ø¬Ø¹ ÙÙ‚Ø· Ù…ØµÙÙˆÙØ© JSON ØµØ­ÙŠØ­Ø© Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ø´Ø±ÙˆØ­.
# #             Ù…Ø«Ø§Ù„:
# #             [
# #               {{
# #                 "id": 1,
# #                 "difficulty": "Ø³Ù‡Ù„",
# #                 "question": "Ù…Ø§ Ù†ØªÙŠØ¬Ø© 2 + 2ØŸ",
# #                 "options": ["3", "4", "5", "6"],
# #                 "correct_answer": "4"
# #               }},
# #               {{
# #                 "id": 2,
# #                 "difficulty": "Ù…ØªÙˆØ³Ø·",
# #                 "question": "Ø­Ù„: 5Ø³ - 7 = 18",
# #                 "options": ["Ø³=3", "Ø³=4", "Ø³=5", "Ø³=6"],
# #                 "correct_answer": "Ø³=5"
# #               }}
# #             ]
# #             """
# #         else:
# #             # English prompt
# #             return f"""
# #             You are an expert exam creator.
# #             Task: Generate {count} UNIQUE and Creative questions for a {exam_type} exam.
# #             Grade: {grade}
# #             Subject: {subject}
# #             âœ… Rules:
# #             - No duplicate or repeated questions.
# #             - Creative and varied concepts for each question.
# #             - Each question must have:
# #               "id": number,
# #               "question": "the question text",
# #               "options": ["A", "B", "C", "D"],
# #               "correct_answer": "the correct option"
# #             - Use **different difficulty levels** (Easy, Medium, Hard).
# #             - Return ONLY a valid JSON array with no explanations.
# #             """
# #     else:
# #         # Evaluate
# #         if lang == "ar":
# #             return f"""
# #             Ù‚Ù… Ø¨ØªÙ‚ÙŠÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ø·Ø§Ù„Ø¨ Ù„Ø§Ù…ØªØ­Ø§Ù† {exam_type}.
# #             Ø§Ù„ØµÙ: {grade}
# #             Ø§Ù„Ù…Ø§Ø¯Ø©: {subject}
# #             Ù…Ø¹Ø±Ù Ø§Ù„Ø·Ø§Ù„Ø¨: {student_id}
# #             Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ø·Ø§Ù„Ø¨: {answers}
# #             Ø£Ø±Ø¬Ø¹ JSON Ø¨Ø§Ù„ØµÙŠØºØ© Ø§Ù„ØªØ§Ù„ÙŠØ©:
# #             {{
# #               "score": <Ø±Ù‚Ù… Ù…Ù† 0-100>,
# #               "feedback": [
# #                 {{
# #                   "question": "...",
# #                   "student_answer": "...",
# #                   "correct_answer": "...",
# #                   "comment": "..."
# #                 }}
# #               ]
# #             }}
# #             """
# #         else:
# #             return f"""
# #             Evaluate student's answers for a {exam_type} exam.
# #             Grade: {grade}
# #             Subject: {subject}
# #             Student ID: {student_id}
# #             Student answers: {answers}
# #             Output JSON in this format:
# #             {{
# #               "score": <number from 0-100>,
# #               "feedback": [
# #                 {{
# #                   "question": "...",
# #                   "student_answer": "...",
# #                   "correct_answer": "...",
# #                   "comment": "..."
# #                 }}
# #               ]
# #             }}
# #             """


# # # ğŸŸ¢ Generate Questions Endpoint
# # @app.route("/generate-questions", methods=["POST"])
# # def generate_questions():
# #     data = request.get_json()
# #     exam_type = data.get("examType", "TIMSS")
# #     grade = data.get("grade", "Grade 8")
# #     subject = data.get("subject", "Math")
# #     count = data.get("count", 5)
# #     lang = data.get("lang", "en")

# #     prompt = build_prompt(exam_type, grade, subject, count, lang=lang, task="generate")

# #     try:
# #         resp = requests.post(
# #             GEMINI_URL,
# #             headers={"Content-Type": "application/json"},
# #             json={"contents": [{"parts": [{"text": prompt}]}]},
# #         )
# #         resp.raise_for_status()

# #         gemini_data = resp.json()
# #         raw_text = gemini_data["candidates"][0]["content"]["parts"][0]["text"]

# #         try:
# #             questions = json.loads(raw_text)
# #         except:
# #             match = re.search(r"\[.*\]", raw_text, re.S)
# #             if match:
# #                 questions = json.loads(match.group(0))
# #             else:
# #                 return jsonify({"error": "Invalid JSON from Gemini", "raw": raw_text}), 500

# #         return jsonify({
# #             "examType": exam_type,
# #             "timeLimit": "40 minutes",
# #             "questions": questions
# #         })

# #     except Exception as e:
# #         return jsonify({"error": "Failed to generate questions", "details": str(e)}), 500


# # # ğŸŸ¢ Evaluate Answers Endpoint
# # @app.route("/evaluate", methods=["POST"])
# # def evaluate():
# #     data = request.get_json()
# #     exam_type = data.get("examType", "TIMSS")
# #     grade = data.get("grade", "Grade 8")
# #     subject = data.get("subject", "Math")
# #     student_id = data.get("studentId", "test_student")
# #     answers = data.get("answers", [])
# #     lang = data.get("lang", "en")

# #     prompt = build_prompt(
# #         exam_type, grade, subject, None,
# #         lang=lang, task="evaluate",
# #         answers=answers, student_id=student_id
# #     )

# #     try:
# #         resp = requests.post(
# #             GEMINI_URL,
# #             headers={"Content-Type": "application/json"},
# #             json={"contents": [{"parts": [{"text": prompt}]}]},
# #         )
# #         resp.raise_for_status()

# #         gemini_data = resp.json()
# #         raw_text = gemini_data["candidates"][0]["content"]["parts"][0]["text"]

# #         try:
# #             feedback_json = json.loads(raw_text)
# #         except:
# #             match = re.search(r"\{.*\}", raw_text, re.S)
# #             if match:
# #                 feedback_json = json.loads(match.group(0))
# #             else:
# #                 feedback_json = {"score": None, "feedback": raw_text}

# #         return jsonify(feedback_json)

# #     except Exception as e:
# #         return jsonify({"error": "Failed to evaluate answers", "details": str(e)}), 500


# # # ğŸŸ¢ Run Flask App
# # if __name__ == "__main__":
# #     app.run(debug=True, port=5000)
# from flask import Flask, request, jsonify
# import requests, os, json, re

# app = Flask(__name__)

# # ğŸ”‘ Gemini API
# GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
# GEMINI_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GEMINI_API_KEY}"


# # ğŸŸ¢ Helper: Build Prompt
# def build_prompt(exam_type, grade, subject, count, lang="en", task="generate", answers=None, student_id=""):
#     if task == "generate":
#         # ========== Arabic Version ==========
#         if lang == "ar":
#             # Normalize exam type
#             exam_type_lower = exam_type.strip().lower()

#             # Context by exam type
#             if "Ø¨ÙŠØ±Ù„Ø²" in exam_type_lower or "pirls" in exam_type_lower:
#                 framework_context = f"""
#                 Ø§Ø®ØªØ¨Ø§Ø± PIRLS ÙŠÙ‚ÙŠØ³ Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ù‚Ø±Ø§Ø¦ÙŠ Ù„Ù„ØµÙ {grade}.
#                 ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù„ÙƒÙ„ Ø§Ø®ØªØ¨Ø§Ø± Ù†Øµ Ù‚ØµÙŠØ± ÙˆØ§Ù‚Ø¹ÙŠ (Ø­ÙˆØ§Ù„ÙŠ 80 Ø¥Ù„Ù‰ 120 ÙƒÙ„Ù…Ø©)ØŒ Ø«Ù… {count} Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ ÙÙ‚Ø·.
#                 """
#             elif "ØªÙŠÙ…Ø³" in exam_type_lower or "timss" in exam_type_lower:
#                 framework_context = f"""
#                 Ø§Ø®ØªØ¨Ø§Ø± TIMSS ÙŠÙ‚ÙŠØ³ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… ÙˆØ§Ù„Ù…Ù‡Ø§Ø±Ø§Øª ÙÙŠ Ù…Ø§Ø¯Ø© {subject} Ù„Ù„ØµÙ {grade}.
#                 Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø±Ù‚Ù…ÙŠØ©ØŒ Ø¹Ù„Ù…ÙŠØ©ØŒ ÙˆØªØºØ·ÙŠ Ù…Ø³ØªÙˆÙŠØ§Øª: Ù…Ø¹Ø±ÙØ©ØŒ ØªØ·Ø¨ÙŠÙ‚ØŒ ÙˆØ§Ø³ØªØ¯Ù„Ø§Ù„.
#                 """
#             elif "Ø¨ÙŠØ²Ø§" in exam_type_lower or "pisa" in exam_type_lower:
#                 framework_context = f"""
#                 Ø§Ø®ØªØ¨Ø§Ø± PISA ÙŠÙ‚ÙŠØ³ Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù†Ù‚Ø¯ÙŠ ÙˆØ­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø§Øª ÙÙŠ Ù…ÙˆØ§Ù‚Ù Ø­ÙŠØ§ØªÙŠØ© ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ø·Ù„Ø§Ø¨ Ø§Ù„ØµÙ {grade}.
#                 Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† ØªØ·Ø¨ÙŠÙ‚ÙŠØ© ÙˆØªØ±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ³ÙŠØ± ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆÙ„ÙŠØ³ Ø§Ù„Ø­ÙØ¸.
#                 """
#             else:
#                 framework_context = f"Ø§Ø®ØªØ¨Ø§Ø± {exam_type} Ù„Ù„ØµÙ {grade} ÙÙŠ Ù…Ø§Ø¯Ø© {subject}."

#             return f"""
#             Ø£Ù†Øª Ø®Ø¨ÙŠØ± ØªØ±Ø¨ÙˆÙŠ Ù…Ø®ØªØµ ÙÙŠ ØªØµÙ…ÙŠÙ… Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª {exam_type}.
#             {framework_context}

#             ğŸ¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:
#             Ø£Ù†Ø´Ø¦ Ø§Ø®ØªØ¨Ø§Ø±Ù‹Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ {count} Ø£Ø³Ø¦Ù„Ø© Ø£ØµÙ„ÙŠØ© ÙˆÙˆØ§Ù‚Ø¹ÙŠØ©ØŒ Ù…Ø±ØªØ¨Ø© Ø­Ø³Ø¨ Ø§Ù„ØµØ¹ÙˆØ¨Ø© (Ø³Ù‡Ù„ØŒ Ù…ØªÙˆØ³Ø·ØŒ ØµØ¹Ø¨).

#             ğŸ§© Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¹Ø§Ù…Ø©:
#             - Ù„Ø§ ØªÙƒØªØ¨ Ø£ÙŠ Ù†Øµ Ø®ÙŠØ§Ù„ÙŠ Ø£Ùˆ ØºÙŠØ± Ù…Ù†Ø·Ù‚ÙŠ.
#             - Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ø¶Ø­Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„ØµÙ {grade}.
#             - Ø§Ø¬Ø¹Ù„ ÙƒÙ„ Ø³Ø¤Ø§Ù„ ÙŠÙ‚ÙŠØ³ Ù…Ù‡Ø§Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· (ÙÙ‡Ù…ØŒ ØªØ­Ù„ÙŠÙ„ØŒ ØªØ·Ø¨ÙŠÙ‚ØŒ Ø§Ø³ØªÙ†ØªØ§Ø¬).
#             - Ù„Ø§ ØªÙƒØ±Ø± Ù†ÙØ³ Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„.
#             - Ø£Ø±Ø¬Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø·ØŒ Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ø´Ø±Ø­ Ø£Ùˆ Ù†Øµ Ø¥Ø¶Ø§ÙÙŠ.

#             âš™ï¸ Ø§Ù„ØµÙŠØºØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:
#             {{
#               "exam_type": "{exam_type}",
#               "grade": "{grade}",
#               "subject": "{subject}",
#               "reading_passage": "Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø§Ù…ØªØ­Ø§Ù† PIRLSØŒ Ø§ÙƒØªØ¨ Ù‡Ù†Ø§ Ù†ØµÙ‹Ø§ Ù‚ØµÙŠØ±Ù‹Ø§ Ù…Ù†Ø§Ø³Ø¨Ù‹Ø§.",
#               "questions": [
#                 {{
#                   "id": 1,
#                   "difficulty": "Ø³Ù‡Ù„",
#                   "question": "Ø§ÙƒØªØ¨ Ù‡Ù†Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ÙˆÙ„ Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠ.",
#                   "options": ["Ø§Ù„Ø®ÙŠØ§Ø± Ø£", "Ø§Ù„Ø®ÙŠØ§Ø± Ø¨", "Ø§Ù„Ø®ÙŠØ§Ø± Ø¬", "Ø§Ù„Ø®ÙŠØ§Ø± Ø¯"],
#                   "correct_answer": "Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„ØµØ­ÙŠØ­",
#                   "skill": "Ø§Ø³Ù… Ø§Ù„Ù…Ù‡Ø§Ø±Ø©"
#                 }},
#                 {{
#                   "id": 2,
#                   "difficulty": "Ù…ØªÙˆØ³Ø·",
#                   "question": "Ø§ÙƒØªØ¨ Ù‡Ù†Ø§ Ø³Ø¤Ø§Ù„Ù‹Ø§ Ù…ØªÙˆØ³Ø· Ø§Ù„ØµØ¹ÙˆØ¨Ø©.",
#                   "options": ["Ø£", "Ø¨", "Ø¬", "Ø¯"],
#                   "correct_answer": "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©",
#                   "skill": "Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø£Ùˆ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚"
#                 }}
#               ]
#             }}

#             Ø£Ø±Ø¬Ø¹ ÙÙ‚Ø· ÙƒØ§Ø¦Ù† JSON ØµØ­ÙŠØ­ Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù†Øµ Ø®Ø§Ø±Ø¬ÙŠ.
#             """

#         # ========== English Version ==========
#         else:
#             exam_type_lower = exam_type.strip().lower()

#             if "pirls" in exam_type_lower:
#                 framework_context = f"""
#                 PIRLS measures reading comprehension for Grade {grade}.
#                 Include one short reading passage (80â€“120 words) and {count} multiple-choice questions strictly based on that text.
#                 """
#             elif "timss" in exam_type_lower:
#                 framework_context = f"""
#                 TIMSS measures studentsâ€™ understanding in {subject} at Grade {grade}.
#                 Questions should assess factual knowledge, reasoning, and application of real math/science concepts.
#                 """
#             elif "pisa" in exam_type_lower:
#                 framework_context = f"""
#                 PISA measures problem-solving, reasoning, and critical thinking in real-world contexts for Grade {grade}.
#                 Focus on data interpretation, logic, and applied literacy.
#                 """
#             else:
#                 framework_context = f"International {exam_type} exam for Grade {grade} in {subject}."

#             return f"""
#             You are an expert educational test designer for {exam_type} international assessments.
#             {framework_context}

#             ğŸ¯ Task:
#             Create {count} high-quality, realistic, curriculum-aligned multiple-choice questions.

#             ğŸ§  Rules:
#             - No imaginary or nonsensical content.
#             - Keep questions aligned with student level.
#             - Every question must assess one clear skill: understanding, reasoning, application, or analysis.
#             - Use diverse difficulty levels (Easy, Medium, Hard).
#             - For PIRLS: include one short reading passage.
#             - Return **only valid JSON**, no commentary.

#             âš™ï¸ Expected JSON structure:
#             {{
#               "exam_type": "{exam_type}",
#               "grade": "{grade}",
#               "subject": "{subject}",
#               "reading_passage": "Include only for PIRLS exams.",
#               "questions": [
#                 {{
#                   "id": 1,
#                   "difficulty": "Easy",
#                   "question": "Write the first question text here.",
#                   "options": ["A", "B", "C", "D"],
#                   "correct_answer": "B",
#                   "skill": "Comprehension"
#                 }},
#                 {{
#                   "id": 2,
#                   "difficulty": "Medium",
#                   "question": "Write the second question text here.",
#                   "options": ["A", "B", "C", "D"],
#                   "correct_answer": "A",
#                   "skill": "Reasoning"
#                 }}
#               ]
#             }}

#             Return JSON only â€” no explanation or description outside of it.
#             """

#     # ========== Evaluation Prompt ==========
#     else:
#         if lang == "ar":
#             return f"""
#             Ù‚Ù… Ø¨ØªÙ‚ÙŠÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ø·Ø§Ù„Ø¨ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± {exam_type}.

#             Ø§Ù„ØµÙ: {grade}
#             Ø§Ù„Ù…Ø§Ø¯Ø©: {subject}
#             Ù…Ø¹Ø±Ù Ø§Ù„Ø·Ø§Ù„Ø¨: {student_id}

#             Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ø·Ø§Ù„Ø¨:
#             {answers}

#             Ø£Ø±Ø¬Ø¹ JSON Ø¨Ø§Ù„ØµÙŠØºØ© Ø§Ù„ØªØ§Ù„ÙŠØ©:
#             {{
#               "score": <Ø±Ù‚Ù… Ù…Ù† 0 Ø¥Ù„Ù‰ 100>,
#               "feedback": [
#                 {{
#                   "question": "...",
#                   "student_answer": "...",
#                   "correct_answer": "...",
#                   "comment": "Ø´Ø±Ø­ Ù‚ØµÙŠØ± Ø¹Ù† ØµØ­Ø© Ø£Ùˆ Ø®Ø·Ø£ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©"
#                 }}
#               ]
#             }}
#             """
#         else:
#             return f"""
#             Evaluate student's answers for the {exam_type} exam.

#             Grade: {grade}
#             Subject: {subject}
#             Student ID: {student_id}

#             Student answers:
#             {answers}

#             Return JSON in this format:
#             {{
#               "score": <number between 0 and 100>,
#               "feedback": [
#                 {{
#                   "question": "...",
#                   "student_answer": "...",
#                   "correct_answer": "...",
#                   "comment": "Brief explanation or correction"
#                 }}
#               ]
#             }}
#             """

   

#     else:  # evaluate
#         if lang == "ar":
#             return f"""
#             Ù‚Ù… Ø¨ØªÙ‚ÙŠÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ø·Ø§Ù„Ø¨ Ù„Ø§Ù…ØªØ­Ø§Ù† {exam_type}.

#             Ø§Ù„ØµÙ: {grade}
#             Ø§Ù„Ù…Ø§Ø¯Ø©: {subject}
#             Ù…Ø¹Ø±Ù Ø§Ù„Ø·Ø§Ù„Ø¨: {student_id}

#             Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ø·Ø§Ù„Ø¨:
#             {answers}

#             Ø£Ø±Ø¬Ø¹ JSON Ø¨Ø§Ù„ØµÙŠØºØ© Ø§Ù„ØªØ§Ù„ÙŠØ©:
#             {{
#               "score": <Ø±Ù‚Ù… Ù…Ù† 0-100>,
#               "feedback": [
#                 {{
#                   "question": "...",
#                   "student_answer": "...",
#                   "correct_answer": "...",
#                   "comment": "..."
#                 }}
#               ]
#             }}
#             """
#         else:
#             return f"""
#             Evaluate student's answers for a {exam_type} exam.

#             Grade: {grade}
#             Subject: {subject}
#             Student ID: {student_id}

#             Student answers:
#             {answers}

#             Output JSON in this format:
#             {{
#               "score": <number from 0-100>,
#               "feedback": [
#                 {{
#                   "question": "...",
#                   "student_answer": "...",
#                   "correct_answer": "...",
#                   "comment": "..."
#                 }}
#               ]
#             }}
#             """


# # ğŸŸ¢ Generate Questions
# @app.route("/generate-questions", methods=["POST"])
# def generate_questions():
#     data = request.get_json()
#     exam_type = data.get("examType", "TIMSS")
#     grade = data.get("grade", "Grade 8")
#     subject = data.get("subject", "Math")
#     count = data.get("count", 5)
#     lang = data.get("lang", "en")  # âœ… pick language

#     prompt = build_prompt(exam_type, grade, subject, count, lang=lang, task="generate")

#     try:
#         resp = requests.post(
#             GEMINI_URL,
#             headers={"Content-Type": "application/json"},
#             json={"contents": [{"parts": [{"text": prompt}]}]},
#         )
#         resp.raise_for_status()
#         gemini_data = resp.json()

#         raw_text = gemini_data["candidates"][0]["content"]["parts"][0]["text"]

#         try:
#             questions = json.loads(raw_text)
#         except:
#             match = re.search(r"\[.*\]", raw_text, re.S)
#             if match:
#                 questions = json.loads(match.group(0))
#             else:
#                 return jsonify({"error": "Invalid JSON from Gemini", "raw": raw_text}), 500

#         return jsonify({
#             "examType": exam_type,
#             "timeLimit": "10 minutes",
#             "questions": questions
#         })

#     except Exception as e:
#         return jsonify({"error": "Failed to generate questions", "details": str(e)}), 500


# # ğŸŸ¢ Evaluate Answers
# @app.route("/evaluate", methods=["POST"])
# def evaluate():
#     data = request.get_json()
#     exam_type = data.get("examType", "TIMSS")
#     grade = data.get("grade", "Grade 8")
#     subject = data.get("subject", "Math")
#     student_id = data.get("studentId", "test_student")
#     answers = data.get("answers", [])
#     lang = data.get("lang", "en")  # âœ… support Arabic feedback

#     prompt = build_prompt(exam_type, grade, subject, None, lang=lang, task="evaluate", answers=answers, student_id=student_id)

#     try:
#         resp = requests.post(
#             GEMINI_URL,
#             headers={"Content-Type": "application/json"},
#             json={"contents": [{"parts": [{"text": prompt}]}]},
#         )
#         resp.raise_for_status()
#         gemini_data = resp.json()
#         raw_text = gemini_data["candidates"][0]["content"]["parts"][0]["text"]

#         try:
#             feedback_json = json.loads(raw_text)
#         except:
#             match = re.search(r"\{.*\}", raw_text, re.S)
#             if match:
#                 feedback_json = json.loads(match.group(0))
#             else:
#                 feedback_json = {"score": None, "feedback": raw_text}

#         return jsonify(feedback_json)

#     except Exception as e:
#         return jsonify({"error": "Failed to evaluate answers", "details": str(e)}), 500


# if __name__ == "__main__":
#     app.run(debug=True, port=5000)
from flask import Flask, request, jsonify
import requests, os, json, re

app = Flask(__name__)

# ğŸ”‘ Gemini API Configuration
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GEMINI_API_KEY}"


# ğŸ§  Helper Function: Build Prompt
def build_prompt(exam_type, grade, subject, count, lang="en", task="generate", answers=None, student_id=""):
    # ---------------------- GENERATION MODE ----------------------
    if task == "generate":
        # Arabic Prompt
        if lang == "ar":
            exam_type_lower = exam_type.strip().lower()

            # Context by exam type
            if "Ø¨ÙŠØ±Ù„Ø²" in exam_type_lower or "pirls" in exam_type_lower:
                    framework_context = f"""
                Ø§Ø®ØªØ¨Ø§Ø± PIRLS Ù‡Ùˆ Ø§Ø®ØªØ¨Ø§Ø± Ø¯ÙˆÙ„ÙŠ Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ù‚Ø±Ø§Ø¦ÙŠ Ù„Ø¯Ù‰ Ø·Ù„Ø§Ø¨ Ø§Ù„ØµÙ {grade}.
                Ø§Ù„Ù‡Ø¯Ù Ù‡Ùˆ Ù‚ÙŠØ§Ø³ Ù‚Ø¯Ø±Ø© Ø§Ù„Ø·Ø§Ù„Ø¨ Ø¹Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ© Ø§Ù„Ù‚ØµÙŠØ±Ø© Ù…Ù† Ø®Ù„Ø§Ù„ Ù‚Ø±Ø§Ø¡Ø© Ù†Øµ Ø«Ù… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„ÙŠÙ‡.

                ğŸ“˜ Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:
                - ÙŠØªÙƒÙˆÙ† Ø§Ù„Ø§Ù…ØªØ­Ø§Ù† Ù…Ù† {count} Ø£Ø³Ø¦Ù„Ø©.
                - ÙƒÙ„ Ø³Ø¤Ø§Ù„ ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ **Ù†Øµ ÙˆØ§Ù‚Ø¹ÙŠ Ù‚ØµÙŠØ± (80â€“120 ÙƒÙ„Ù…Ø©)** Ù…ÙƒØªÙˆØ¨ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø­Ù‚Ù„ "reading_passage".
                - Ø¨Ø¹Ø¯ ÙƒÙ„ Ù†ØµØŒ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‡Ù†Ø§Ùƒ **Ø³Ø¤Ø§Ù„ ÙˆØ§Ø­Ø¯** ÙŠØ¹ØªÙ…Ø¯ ÙƒÙ„ÙŠÙ‹Ø§ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ ÙÙ‚Ø·.
                - Ù„Ø§ ÙŠØ¬ÙˆØ² Ø·Ø±Ø­ Ø£ÙŠ Ø³Ø¤Ø§Ù„ Ø¨Ø¯ÙˆÙ† Ù†Øµ Ù‚Ø±Ø§Ø¡Ø© ÙŠØ³Ø¨Ù‚Ù‡.
                - ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø£Ø±Ø¨Ø¹Ø© Ù…Ù†Ø·Ù‚ÙŠØ© ÙˆÙ…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù†Øµ ÙÙ‚Ø·.

                ğŸ”¹ ØªØ¹Ù„ÙŠÙ…Ø§Øª ØµØ§Ø±Ù…Ø©:
                - Ø§Ø¨Ø¯Ø£ Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¨ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†Øµ ÙÙŠ Ø§Ù„Ø­Ù‚Ù„ "reading_passage".
                - Ù„Ø§ ØªÙƒØªØ¨ Ø£Ø³Ø¦Ù„Ø© Ø£Ùˆ Ø®ÙŠØ§Ø±Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„Ù†Øµ.
                - Ø§Ù„Ù†ØµÙˆØµ ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† ÙˆØ§Ù‚Ø¹ÙŠØ© ÙˆÙ…Ù†Ø§Ø³Ø¨Ø© Ù„Ø¹Ù…Ø± Ø·Ù„Ø§Ø¨ Ø§Ù„ØµÙ {grade}.
                - Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø®ÙŠØ§Ù„ØŒ ÙˆÙ„Ø§ Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ­Ø¯Ø«Ø©ØŒ ÙˆÙ„Ø§ Ø£Ø³Ù…Ø§Ø¡ Ø®ÙŠØ§Ù„ÙŠØ©.
                - Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ù…ØªÙ†ÙˆØ¹Ø© ÙÙŠ Ø§Ù„Ù…Ù‡Ø§Ø±Ø§Øª: Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„Ø±Ø¦ÙŠØ³Ø©ØŒ Ø§Ù„ØªÙØ§ØµÙŠÙ„ØŒ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠØŒ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ØŒ Ø§Ù„ØºØ±Ø¶.

                âš™ï¸ Ø§Ù„ØµÙŠØºØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© (JSON ÙÙ‚Ø·):
                {{
                "exam_type": "PIRLS",
                "grade": "{grade}",
                "subject": "{subject}",
                "questions": [
                    {{
                    "id": 1,
                    "reading_passage": "Ø§ÙƒØªØ¨ Ù‡Ù†Ø§ Ù†ØµÙ‹Ø§ ÙˆØ§Ù‚Ø¹ÙŠÙ‹Ø§ Ù‚ØµÙŠØ±Ù‹Ø§ (80â€“120 ÙƒÙ„Ù…Ø©) Ù…Ù†Ø§Ø³Ø¨Ù‹Ø§ Ù„Ù„ØµÙ {grade}.",
                    "question": "Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ù‹Ø§ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø£Ø¹Ù„Ø§Ù‡ ÙÙ‚Ø·.",
                    "options": ["Ø§Ù„Ø®ÙŠØ§Ø± Ø£", "Ø§Ù„Ø®ÙŠØ§Ø± Ø¨", "Ø§Ù„Ø®ÙŠØ§Ø± Ø¬", "Ø§Ù„Ø®ÙŠØ§Ø± Ø¯"],
                    "correct_answer": "Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„ØµØ­ÙŠØ­",
                    "skill": "Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„Ø±Ø¦ÙŠØ³Ø© Ø£Ùˆ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø£Ùˆ Ø§Ù„Ù…Ø¹Ù†Ù‰"
                    }},
                    {{
                    "id": 2,
                    "reading_passage": "Ø§ÙƒØªØ¨ Ù†ØµÙ‹Ø§ Ø¬Ø¯ÙŠØ¯Ù‹Ø§ ÙˆØ§Ù‚Ø¹ÙŠÙ‹Ø§ Ø¢Ø®Ø± (80â€“120 ÙƒÙ„Ù…Ø©).",
                    "question": "Ø³Ø¤Ø§Ù„ Ø¬Ø¯ÙŠØ¯ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ø«Ø§Ù†ÙŠ.",
                    "options": ["...", "...", "...", "..."],
                    "correct_answer": "...",
                    "skill": "Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø£Ùˆ Ø§Ù„ØºØ±Ø¶"
                    }}
                ]
                }}

                âœ³ï¸ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù…Ù‡Ù…Ø©:
                - ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ ÙƒÙ„ Ø³Ø¤Ø§Ù„ Ø¹Ù„Ù‰ Ù†Øµ Ù‚Ø±Ø§Ø¡Ø© ÙÙŠ Ø§Ù„Ø­Ù‚Ù„ "reading_passage".
                - Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù†ØµØŒ ÙØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØºÙŠØ± ØµØ§Ù„Ø­Ø©.
                - Ø£Ø±Ø¬Ø¹ ÙÙ‚Ø· JSON Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ø´Ø±Ø­ Ø®Ø§Ø±Ø¬ÙŠ.
                """




            elif "ØªÙŠÙ…Ø³" in exam_type_lower or "timss" in exam_type_lower:
                framework_context = f"""
                Ø§Ø®ØªØ¨Ø§Ø± TIMSS ÙŠÙ‚ÙŠØ³ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… ÙˆØ§Ù„Ù…Ù‡Ø§Ø±Ø§Øª ÙÙŠ Ù…Ø§Ø¯Ø© {subject} Ù„Ù„ØµÙ {grade}.
                ÙŠØ¬Ø¨ Ø£Ù† ØªØ´Ù…Ù„ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ù…ÙØ§Ù‡ÙŠÙ… Ø±ÙŠØ§Ø¶ÙŠØ© Ø£Ùˆ Ø¹Ù„Ù…ÙŠØ© Ø­Ù‚ÙŠÙ‚ÙŠØ© ÙˆØªØºØ·ÙŠ Ù…Ø³ØªÙˆÙŠØ§Øª:
                - Ø§Ù„Ù…Ø¹Ø±ÙØ©
                - Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
                - Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
                """

            elif "Ø¨ÙŠØ²Ø§" in exam_type_lower or "pisa" in exam_type_lower:
                framework_context = f"""
                Ø§Ø®ØªØ¨Ø§Ø± PISA ÙŠÙ‚ÙŠØ³ Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù†Ù‚Ø¯ÙŠ ÙˆØ­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø§Øª ÙÙŠ Ù…ÙˆØ§Ù‚Ù Ø­ÙŠØ§ØªÙŠØ© ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ø·Ù„Ø§Ø¨ Ø§Ù„ØµÙ {grade}.
                Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† ØªØ·Ø¨ÙŠÙ‚ÙŠØ© ÙˆØªØ±Ø¨Ø· Ø¨ÙŠÙ† Ø§Ù„ØªØ¹Ù„Ù… ÙˆØ§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„ÙŠÙˆÙ…ÙŠ.
                """

            else:
                framework_context = f"Ø§Ø®ØªØ¨Ø§Ø± {exam_type} Ù„Ù„ØµÙ {grade} ÙÙŠ Ù…Ø§Ø¯Ø© {subject}."

            # Arabic Prompt Template
            return f"""
            Ø£Ù†Øª Ø®Ø¨ÙŠØ± ØªØ±Ø¨ÙˆÙŠ Ù…Ø®ØªØµ ÙÙŠ ØªØµÙ…ÙŠÙ… Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª {exam_type}.
            {framework_context}

            ğŸ¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:
            Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø®ØªØ¨Ø§Ø± ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ {count} Ø£Ø³Ø¦Ù„Ø© Ø£ØµÙ„ÙŠØ© ÙˆÙˆØ§Ù‚Ø¹ÙŠØ©ØŒ Ù…Ø±ØªØ¨Ø© Ø­Ø³Ø¨ Ø§Ù„ØµØ¹ÙˆØ¨Ø© (Ø³Ù‡Ù„ØŒ Ù…ØªÙˆØ³Ø·ØŒ ØµØ¹Ø¨).

            ğŸ§© Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¹Ø§Ù…Ø©:
            - Ù„Ø§ ØªÙƒØªØ¨ Ø£ÙŠ Ù†Øµ Ø®ÙŠØ§Ù„ÙŠ Ø£Ùˆ ØºÙŠØ± Ù…Ù†Ø·Ù‚ÙŠ.
            - Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„ØµÙ {grade}.
            - Ø§Ø¬Ø¹Ù„ ÙƒÙ„ Ø³Ø¤Ø§Ù„ ÙŠÙ‚ÙŠØ³ Ù…Ù‡Ø§Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· (ÙÙ‡Ù…ØŒ ØªØ­Ù„ÙŠÙ„ØŒ ØªØ·Ø¨ÙŠÙ‚ØŒ Ø§Ø³ØªÙ†ØªØ§Ø¬).
            - Ù„Ø§ ØªÙƒØ±Ø± Ù†ÙØ³ Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„.
            - Ø£Ø±Ø¬Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ø´Ø±Ø­ Ø¥Ø¶Ø§ÙÙŠ.

            âš™ï¸ Ø§Ù„ØµÙŠØºØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:
            {{
              "exam_type": "{exam_type}",
              "grade": "{grade}",
              "subject": "{subject}",
              "reading_passage": "Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø§Ù…ØªØ­Ø§Ù† PIRLSØŒ Ø§ÙƒØªØ¨ Ù‡Ù†Ø§ Ù†ØµÙ‹Ø§ Ù‚ØµÙŠØ±Ù‹Ø§ (80â€“120 ÙƒÙ„Ù…Ø©).",
              "questions": [
                {{
                  "id": 1,
                  "difficulty": "Ø³Ù‡Ù„",
                  "question": "Ø§ÙƒØªØ¨ Ù‡Ù†Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ÙˆÙ„ Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠ.",
                  "options": ["Ø§Ù„Ø®ÙŠØ§Ø± Ø£", "Ø§Ù„Ø®ÙŠØ§Ø± Ø¨", "Ø§Ù„Ø®ÙŠØ§Ø± Ø¬", "Ø§Ù„Ø®ÙŠØ§Ø± Ø¯"],
                  "correct_answer": "Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„ØµØ­ÙŠØ­",
                  "skill": "Ø§Ø³Ù… Ø§Ù„Ù…Ù‡Ø§Ø±Ø©"
                }},
                {{
                  "id": 2,
                  "difficulty": "Ù…ØªÙˆØ³Ø·",
                  "question": "Ø§ÙƒØªØ¨ Ù‡Ù†Ø§ Ø³Ø¤Ø§Ù„Ù‹Ø§ Ù…ØªÙˆØ³Ø· Ø§Ù„ØµØ¹ÙˆØ¨Ø©.",
                  "options": ["Ø£", "Ø¨", "Ø¬", "Ø¯"],
                  "correct_answer": "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©",
                  "skill": "Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø£Ùˆ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚"
                }}
              ]
            }}

            Ø£Ø±Ø¬Ø¹ ÙÙ‚Ø· ÙƒØ§Ø¦Ù† JSON ØµØ§Ù„Ø­ Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù†Øµ Ø®Ø§Ø±Ø¬ÙŠ.
            """

        # English Prompt
        else:
            exam_type_lower = exam_type.strip().lower()

            if "pirls" in exam_type_lower:
                framework_context = f"""
                PIRLS (Progress in International Reading Literacy Study) assesses reading comprehension
                for Grade {grade} students through short, realistic passages followed by comprehension questions.

                ğŸ“˜ Structure of the exam:
                1. Include one short passage (80â€“120 words) suitable for Grade {grade}.
                   It can be narrative (story) or informational, but must be realistic and meaningful.
                2. Include {count} multiple-choice questions directly based on the passage.

                ğŸ¯ Each question should assess a specific reading skill:
                - Main idea identification
                - Supporting detail recognition
                - Vocabulary meaning in context
                - Inference and reasoning
                - Authorâ€™s purpose
                """

            elif "timss" in exam_type_lower:
                framework_context = f"""
                TIMSS (Trends in International Mathematics and Science Study) measures
                studentsâ€™ understanding of mathematics and science at Grade {grade}.
                Questions should test factual knowledge, reasoning, and problem-solving
                using real-world examples.
                """

            elif "pisa" in exam_type_lower:
                framework_context = f"""
                PISA (Programme for International Student Assessment) measures studentsâ€™
                ability to apply knowledge and skills to real-life situations.
                Focus on data interpretation, logical reasoning, and problem-solving.
                """

            else:
                framework_context = f"International {exam_type} exam for Grade {grade} in {subject}."

            # English Prompt Template
            return f"""
            You are an expert educational test designer for {exam_type} assessments.
            {framework_context}

            ğŸ¯ Task:
            Create {count} high-quality, realistic, and curriculum-aligned multiple-choice questions.

            ğŸ§  Rules:
            - Avoid fictional or illogical content.
            - Keep all questions at an appropriate grade level.
            - Each question should measure one skill (understanding, reasoning, application, analysis).
            - Vary difficulty levels (Easy, Medium, Hard).
            - For PIRLS: include one short reading passage (80â€“120 words).
            - Return **only valid JSON**, with no extra commentary.

            âš™ï¸ Expected JSON structure:
            {{
              "exam_type": "{exam_type}",
              "grade": "{grade}",
              "subject": "{subject}",
              "reading_passage": "Include only for PIRLS exams.",
              "questions": [
                {{
                  "id": 1,
                  "difficulty": "Easy",
                  "question": "Write the first question text here.",
                  "options": ["A", "B", "C", "D"],
                  "correct_answer": "B",
                  "skill": "Comprehension"
                }},
                {{
                  "id": 2,
                  "difficulty": "Medium",
                  "question": "Write the second question text here.",
                  "options": ["A", "B", "C", "D"],
                  "correct_answer": "A",
                  "skill": "Reasoning"
                }}
              ]
            }}

            Return JSON only â€” no explanations or extra text.
            """

    # ---------------------- EVALUATION MODE ----------------------
    else:
        if lang == "ar":
            return f"""
            Ù‚Ù… Ø¨ØªÙ‚ÙŠÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ø·Ø§Ù„Ø¨ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± {exam_type}.

            Ø§Ù„ØµÙ: {grade}
            Ø§Ù„Ù…Ø§Ø¯Ø©: {subject}
            Ù…Ø¹Ø±Ù Ø§Ù„Ø·Ø§Ù„Ø¨: {student_id}

            Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ø·Ø§Ù„Ø¨:
            {answers}

            Ø£Ø±Ø¬Ø¹ JSON Ø¨Ø§Ù„ØµÙŠØºØ© Ø§Ù„ØªØ§Ù„ÙŠØ©:
            {{
              "score": <Ø±Ù‚Ù… Ù…Ù† 0 Ø¥Ù„Ù‰ 100>,
              "feedback": [
                {{
                  "question": "...",
                  "student_answer": "...",
                  "correct_answer": "...",
                  "comment": "Ø´Ø±Ø­ Ù‚ØµÙŠØ± Ø¹Ù† ØµØ­Ø© Ø£Ùˆ Ø®Ø·Ø£ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©"
                }}
              ]
            }}
            """
        else:
            return f"""
            Evaluate the student's answers for the {exam_type} exam.

            Grade: {grade}
            Subject: {subject}
            Student ID: {student_id}

            Student answers:
            {answers}

            Return JSON in the following format:
            {{
              "score": <number between 0 and 100>,
              "feedback": [
                {{
                  "question": "...",
                  "student_answer": "...",
                  "correct_answer": "...",
                  "comment": "Brief explanation or feedback"
                }}
              ]
            }}
            """


# ğŸŸ¢ Endpoint: Generate Questions
@app.route("/generate-questions", methods=["POST"])
def generate_questions():
    data = request.get_json()
    exam_type = data.get("examType", "TIMSS")
    grade = data.get("grade", "Grade 8")
    subject = data.get("subject", "Math")
    count = data.get("count", 5)
    lang = data.get("lang", "en")

    prompt = build_prompt(exam_type, grade, subject, count, lang=lang, task="generate")

    try:
        resp = requests.post(
            GEMINI_URL,
            headers={"Content-Type": "application/json"},
            json={"contents": [{"parts": [{"text": prompt}]}]},
            timeout=60
        )
        resp.raise_for_status()
        gemini_data = resp.json()
        raw_text = gemini_data["candidates"][0]["content"]["parts"][0]["text"]

        try:
            questions = json.loads(raw_text)
        except:
            match = re.search(r"\[.*\]", raw_text, re.S)
            if match:
                questions = json.loads(match.group(0))
            else:
                return jsonify({"error": "Invalid JSON from Gemini", "raw": raw_text}), 500

        return jsonify({
            "examType": exam_type,
            "timeLimit": "10 minutes",
            "questions": questions
        })

    except Exception as e:
        return jsonify({"error": "Failed to generate questions", "details": str(e)}), 500


# ğŸŸ¢ Endpoint: Evaluate Answers
@app.route("/evaluate", methods=["POST"])
def evaluate():
    data = request.get_json()
    exam_type = data.get("examType", "TIMSS")
    grade = data.get("grade", "Grade 8")
    subject = data.get("subject", "Math")
    student_id = data.get("studentId", "test_student")
    answers = data.get("answers", [])
    lang = data.get("lang", "en")

    prompt = build_prompt(exam_type, grade, subject, None, lang=lang,
                          task="evaluate", answers=answers, student_id=student_id)

    try:
        resp = requests.post(
            GEMINI_URL,
            headers={"Content-Type": "application/json"},
            json={"contents": [{"parts": [{"text": prompt}]}]},
            timeout=60
        )
        resp.raise_for_status()
        gemini_data = resp.json()
        raw_text = gemini_data["candidates"][0]["content"]["parts"][0]["text"]

        try:
            feedback_json = json.loads(raw_text)
        except:
            match = re.search(r"\{.*\}", raw_text, re.S)
            if match:
                feedback_json = json.loads(match.group(0))
            else:
                feedback_json = {"score": None, "feedback": raw_text}

        return jsonify(feedback_json)

    except Exception as e:
        return jsonify({"error": "Failed to evaluate answers", "details": str(e)}), 500


# ğŸ Run Application
if __name__ == "__main__":
    app.run(debug=True, port=5000)
